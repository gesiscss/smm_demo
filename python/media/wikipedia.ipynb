{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Media Outlets Activity on Wikipedia aggregated by Media Type\n",
    "\n",
    "The parameters in the cell below can be adjusted to explore other media outlets and time frames.\n",
    "\n",
    "### How to explore other media types?\n",
    "The ***media*** parameter can be use to aggregate media outlets by their type. The column `subcategory` in this [this other notebook](../media.ipynb?autorun=true) show the media outlets that belong each type.\n",
    "\n",
    "***Alternatively***, you can direcly use the [organizations API](http://mediamonitoring.gesis.org/api/organizations/swagger/), or access it with the [SMM Wrapper](https://pypi.org/project/smm-wrapper/).\n",
    "\n",
    "## A. Set Up parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters: \n",
    "media = 'TV'\n",
    "from_date = '2017-09-01'\n",
    "to_date = '2018-12-31'\n",
    "aggregation = 'week'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blacklist = [\n",
    "#media\n",
    "'Rbb 88.8', 'Deutsche Wirtschaftsnachrichten', 'Hessische/Niedersächsische Allgemeine', \n",
    "'Mediengruppe Straubinger Tagblatt/Landshuter Zeitung', 'Niedersächsische Allgemeine', \n",
    "'Sindelfinger Zeitung/Böblinger Zeitung', 'Rbb 88.8', 'Rbb24']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Using APIs\n",
    "### B.1 Using the SMM Organization API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from smm_wrapper import SMMOrganizations\n",
    "\n",
    "# Create an instance to the smm wrapper\n",
    "smm = SMMOrganizations()\n",
    "\n",
    "# Request the politicians from the API\n",
    "df = smm.dv.get_organizations()\n",
    "df=df[~df['name'].isin(blacklist)]\n",
    "df = df[(df['category']=='media')]\n",
    "\n",
    "# Filter the accounts by party, and valid ones (the ones that contain wp_ids)\n",
    "media_df = (df[(df['subcategory'].str.contains(media)) & (df['wp_ids'].notnull())]) if media != 'All' else df[df['wp_ids'].notnull()]\n",
    "\n",
    "# query the Social Media Monitoring API\n",
    "if len(media_df)!= 0:\n",
    "    wiki_chobs = pd.concat(smm.dv.wikipedia(_id=organization_id, from_date=from_date, to_date=to_date, aggregate_by=aggregation) \n",
    "               for organization_id in media_df.index)\n",
    "    wiki_chobs = wiki_chobs.groupby('date').agg({'chobs': 'sum'}).reset_index()\n",
    "else:\n",
    "    print(\"No data for this wikipedia account found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.2 Using the Wikiwho API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wikiwho_wrapper import WikiWho\n",
    "\n",
    "#using wikiwho to extract conflicts and revisions\n",
    "ww = WikiWho(lng='de')\n",
    "edit_persistance_gen = (\n",
    "    ww.dv.edit_persistence(page_id=wp_id , start=from_date, end=to_date) \n",
    "    for wp_ids in media_df['wp_ids'] for wp_id in wp_ids)\n",
    "wiki_data = pd.concat(df for df in edit_persistance_gen if len(df) > 0)\n",
    "wiki_data['undos'] = wiki_data['dels'] + wiki_data['reins']\n",
    "wiki_data['date'] = pd.to_datetime(wiki_data['year_month'])\n",
    "wiki_data = wiki_data.groupby('date')['conflict','elegibles','undos'].sum().reset_index()\n",
    "wiki_data['conflict_score'] = wiki_data['conflict'] / wiki_data['elegibles']\n",
    "wiki_data.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.3 Using the Wikimedia API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.parse\n",
    "\n",
    "# open a session\n",
    "session = requests.Session()\n",
    "\n",
    "# prepare url\n",
    "vurl = (\"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article\"\n",
    "        \"/de.wikipedia.org/all-access/user/{wp_title}/daily/\"\n",
    "        f\"{from_date.replace('-','')}/{to_date.replace('-','')}\")\n",
    "\n",
    "# cleaning method for failed responses of wikimedia Views\n",
    "def clean_json(_json):\n",
    "    \"\"\" Complete the json with and empty `items` key when the wikimedia\n",
    "    views fails to find the article\n",
    "    \"\"\"\n",
    "    if 'items' not in _json:\n",
    "        _json['items'] = []\n",
    "    return _json\n",
    "        \n",
    "# use the wikimedia API to download the views\n",
    "views = pd.concat([pd.DataFrame(clean_json(session.get(url=vurl.format(\n",
    "           wp_title=urllib.parse.quote(wp_title, safe=''))).json())['items']) \n",
    "           for wp_titles in media_df['wp_titles'] for wp_title in wp_titles])\n",
    "views['timestamp']=pd.to_datetime(views['timestamp'], format='%Y%m%d%H')\n",
    "\n",
    "# weekly or monthly aggregation of the data\n",
    "if aggregation  == 'week':    \n",
    "    views = views.groupby([pd.Grouper(key='timestamp', freq='W-SUN')])['views'].sum().reset_index().sort_values('timestamp')\n",
    "    views['timestamp'] = views['timestamp'] - pd.Timedelta(days=6)\n",
    "    \n",
    "elif aggregation  == 'month':    \n",
    "    views = views.groupby([pd.Grouper(key='timestamp', freq='MS')])['views'].sum().reset_index().sort_values('timestamp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Plotting\n",
    "### C.1 Plot Wikipedia Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly\n",
    "from plotly import graph_objs as go\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "\n",
    "plotly.offline.iplot({\n",
    "    \"data\": [go.Scatter(x=views['timestamp'], y=views['views'], name='Views'),\n",
    "             go.Scatter(x=wiki_chobs['date'], y=wiki_chobs['chobs'], name='Changes', yaxis='y2')], \n",
    "    \"layout\": go.Layout( title='Wikipedia Activity', yaxis=dict(title='Views'),\n",
    "    yaxis2=dict(title='Changes', overlaying='y', side='right'))\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C.2 Plot Wikipedia Disagreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotly.offline.iplot({\n",
    "    \"data\": [go.Scatter(x=wiki_data['date'], y=wiki_data['undos'], name='Undos', line_shape='spline'),\n",
    "            go.Scatter(x=wiki_data['date'], y=wiki_data['conflict_score'], name='Conflict', line_shape='spline', yaxis='y2')], \n",
    "    \"layout\": go.Layout(title='Wikipedia Disagreement', yaxis=dict(title='Undos'),\n",
    "    yaxis2=dict(title='Conflict', overlaying='y',side='right'))\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
